{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef130b84-2f40-4d4a-b193-6289cb4a4523",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark\n",
    "\n",
    "### Agenda \n",
    " \n",
    "1. Introduccion\n",
    "2. Arquitectura\n",
    "3. Tipos de Clusters\n",
    "4. Modulos de Spark\n",
    "5. Spark RDD\n",
    "6. Spark DataFrame\n",
    "7. Spark SQL\n",
    "8. Spark Streaming\n",
    "9. Spark GrpahFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e634f9-8165-4596-bd31-f5e05eebde19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduccion\n",
    "\n",
    "PySpark es una biblioteca de Spark escrita en Python para ejecutar aplicaciones de Python usando las capacidades de Apache Spark, con PySpark podemos ejecutar aplicaciones en paralelo en el clúster distribuido (múltiples nodos).\n",
    "En otras palabras, PySpark es una API de Python para Apache Spark. Apache Spark es un motor de procesamiento analítico para potentes aplicaciones de aprendizaje automático y procesamiento de datos distribuidos a gran escala.\n",
    "\n",
    "##### ¿Quién utiliza Pyspark?\n",
    "PySpark se usa muy bien en la comunidad de ciencia de datos y aprendizaje automático, ya que hay muchas bibliotecas de ciencia de datos ampliamente utilizadas escritas en Python, incluidas NumPy, TensorFlow. También se utiliza debido a su procesamiento eficiente de grandes conjuntos de datos. PySpark ha sido utilizado por muchas organizaciones como Walmart, Trivago, Sanofi, Runtastic y muchas más.\n",
    "\n",
    "##### caracterísicas importantes de Spark\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/08/pyspark-features-1.png)\n",
    "# \n",
    "- Cálculo en memoria\n",
    "- Procesamiento distribuido usando paralelizar\n",
    "- Se puede usar con muchos administradores de clústeres (Spark, Yarn, Mesos, etc.)\n",
    "- Tolerante a fallos\n",
    "- Inmutable\n",
    "- Evaluación perezosa\n",
    "- Caché y persistencia\n",
    "- Optimización incorporada al usar DataFrames\n",
    "- Soporta ANSI SQL\n",
    "\n",
    "##### Ventajas\n",
    "#\n",
    "- Spark es un motor de procesamiento distribuido, en memoria y de uso general que le permite procesar datos de manera eficiente y distribuida.\n",
    "- Obtendrá grandes beneficios al usar Spark para canalizaciones de ingesta de datos.\n",
    "- Con Spark podemos procesar datos de Hadoop HDFS, AWS S3 y muchos sistemas de archivos.\n",
    "- PySpark también se usa para procesar datos en tiempo real usando Streaming y Kafka.\n",
    "- Con spark streaming, también puede transmitir archivos desde el sistema de archivos y también desde el socket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc13b97c-6e22-49ca-b945-efd7522cb938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Arquitectura\n",
    "# \n",
    "Apache Spark funciona en una arquitectura master-slave donde el master se llama \"driver\" y los esclavos se llaman \"Workers\". Cuando ejecuta una aplicación Spark, Spark Driver crea un contexto que es un punto de entrada a su aplicación, y todas las operaciones (transformaciones y acciones) se ejecutan en los workers y los recursos son administrados por el Administrador de clústeres.\n",
    "\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/02/spark-cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c710ab46-0bd0-4e2e-94a8-d573e24ace0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tipos de clusters\n",
    "#\n",
    "- Standalone –  un administrador de clúster simple incluido con Spark que facilita la configuración de un clúster.\n",
    "- Apache Mesos – Mesos es un administrador de clústeres que también puede ejecutar aplicaciones Hadoop MapReduce y PySpark.\n",
    "- Hadoop YARN –  el administrador de recursos en Hadoop 2. Esto se usa principalmente como administrador de clústeres.\n",
    "- Kubernetes – un sistema de código abierto para automatizar la implementación, el escalado y la gestión de aplicaciones en contenedores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08b3519-519b-48ff-ac3d-49a903e3d192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Módulos de Spark\n",
    "\n",
    "![](https://tse1.mm.bing.net/th?id=OIP.IH66oypTYnWQjFkFHoBMCgHaD2&pid=Api&P=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fbe660-9e52-46cd-b766-f32fe0269c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RDD\n",
    "\n",
    "RDD es una estructura de datos fundamental de Spark que es una colección de objetos distribuidos inmutables y tolerantes a fallas, lo que significa que una vez que crea un RDD no puede cambiarlo. Cada conjunto de datos en RDD se divide en particiones lógicas, que se pueden calcular en diferentes nodos del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ae8225-89ee-400e-a966-17fbea3f26e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.9/site-packages (from pyspark) (0.10.9.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2d401e-1301-46b2-8266-032be599e883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"RDD\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2afa44-0561-4034-8330-b0bc9d7ac31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creacion de un RDD utilizando parallelize\n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "rdd.collect() #accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b7fcec-8c9f-4626-94ff-ec2271aa8adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:306)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:334)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:233)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:301)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:297)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:301)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:297)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input path does not exist: hdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:279)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creación de un RDD utilizando un archivo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rdd2 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrdd2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#accion\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/core/rdd.py:1700\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:306)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:245)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:334)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:233)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:301)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:297)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:301)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:297)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input path does not exist: hdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:279)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "# Creación de un RDD utilizando un archivo\n",
    "rdd2 = spark.sparkContext.textFile(\"hdfs://namenode:8020/user/raw/mysql/bd_Wilson/t_student_mat\")\n",
    "rdd2.collect() #accion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e149a9ad-01dc-4439-9d37-6fa33eb064e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Operaciones RDD\n",
    "En PySpark RDD, puede realizar dos tipos de operaciones.\n",
    "\n",
    "Transformaciones RDD:  las transformaciones son operaciones perezosas. Cuando ejecuta una transformación (por ejemplo, una actualización), en lugar de actualizar un RDD actual, estas operaciones devuelven otro RDD.\n",
    "\n",
    "Acciones de RDD  : operaciones que activan el cálculo y devuelven valores de RDD al controlador.\n",
    "\n",
    "##### Transformaciones RDD\n",
    "Las transformaciones en Spark RDD  devuelven otro RDD y las transformaciones son perezosas, lo que significa que no se ejecutan hasta que llamas a una acción en RDD. Algunas transformaciones en los RDD son flatMap(), map(), reduceByKey(), filter(), sortByKey() y devuelven un nuevo RDD en lugar de actualizar el actual.\n",
    "\n",
    "##### Acciones de RDD\n",
    "La operación Acción de RDD devuelve los valores de un RDD a un nodo de controlador. En otras palabras, cualquier función RDD que no devuelva RDD[T] se considera una acción. \n",
    "\n",
    "Algunas acciones en RDD son count(), collect(), first(), max()y reduce() y más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d20386e9-2c95-4d49-8bc1-8f9d3e6453bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "DataFrame es una colección distribuida de datos organizados en columnas con nombre. Es conceptualmente equivalente a una tabla en una base de datos relacional o un marco de datos en R/Python, pero con optimizaciones más ricas bajo el capó. Los marcos de datos se pueden construir a partir de una amplia gama de fuentes, como archivos de datos estructurados, tablas en Hive, bases de datos externas o RDD existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8403872-1c81-46f8-a42c-d9c266d0a785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un dataframe utilizando createDataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6945d0-6198-4ffc-bc14-a9bd7f1bb7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7777187f-9a83-4e9c-bbcd-5d37015180a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creación de un dataframe utilizando una fuente externa\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/resources/zipcodes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# spark.read.json/ .parquet ( spark.read.format().load())\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Creación de un dataframe utilizando una fuente externa\n",
    "df = spark.read.csv(\"/tmp/resources/zipcodes.csv\") # spark.read.json/ .parquet ( spark.read.format().load())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6639e965-86d4-4f59-8115-5588db9c5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "Spark SQL en Apache Spark es uno de los módulos para el procesamiento de la información que ofrece Apache Spark y que trabaja con datos estructurados.\n",
    "Una vez que haya creado un DataFrame, puede interactuar con los datos utilizando la sintaxis SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0619ae59-15c7-44ec-b3e7-71c2d89cbfb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df.cache()\n",
    "df.count()\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0145ee84-c4b0-4da5-a1f4-0a9304cd5718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark Streaming\n",
    "Spark Streaming es un sistema de procesamiento de transmisión escalable, de alto rendimiento y tolerante a fallas que admite cargas de trabajo tanto por lotes como de transmisión. Se utiliza para procesar datos en tiempo real de fuentes como la carpeta del sistema de archivos, el socket TCP, S3 , Kafka , Flume , Twitter y Amazon Kinesis , por nombrar algunos. Los datos procesados se pueden enviar a bases de datos, Kafka, dashboard, etc.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12f7e519-1b2a-4254-af0e-fedc5c66962a",
     "showTitle": true,
     "title": "Streaming con socket"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\n",
    "      .format(\"socket\")\n",
    "      .option(\"host\",\"localhost\")\n",
    "      .option(\"port\",\"9090\")\n",
    "      .load()\n",
    "        \n",
    "query = df.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"updated\")\n",
    "      .start()\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7725bf95-6aba-4ed7-be82-42d4aacd1ed2",
     "showTitle": true,
     "title": "Streaming con kafka"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "        .option(\"subscribe\", \"json_topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\") // From starting\n",
    "        .load()\n",
    "        \n",
    "df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "   .writeStream\n",
    "   .format(\"kafka\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "   .option(\"topic\", \"josn_data_topic\")\n",
    "   .start()\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79195a12-e27e-4dbf-b8db-0ab2a2787cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark GrpahFrame\n",
    "\n",
    "GraphFrames es un paquete para Apache Spark que proporciona gráficos basados ​​en DataFrame. Proporciona API de alto nivel en Scala, Java y Python. Su objetivo es proporcionar tanto la funcionalidad de GraphX como la funcionalidad extendida aprovechando Spark DataFrames. Esta funcionalidad ampliada incluye búsqueda de motivos, serialización basada en DataFrame y consultas gráficas altamente expresivas."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Introduccion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
